
# Taxi Data ELT & Streaming Pipeline Project

This project implements a full data engineering workflow to process data using ELT (Extract, Load, Transform) and real-time streaming pipelines.

---

## Project Goals

### Batch Pipeline (ELT)
- Transfer taxi trip data from a local source to BigQuery via GCS staging.
- **GCS Staging Bucket:**
- **BigQuery Dataset (Staging & Mart):** 
- Use Airflow as a daily scheduler to orchestrate extractor, loader, and transformer tasks.
- Leverage scripts (extractor, loader, transformer) using Airflow Operators.
- Partition and incrementally load taxi data in BigQuery.

### Streaming Pipeline
- Stream simulated taxi trip data in real time using:
  - **Pub/Sub** for messaging
  - **Dataflow** for (optional) real-time transformation
  - **BigQuery** for real-time data warehousing
- A custom Python script simulates and publishes the data.

---

## Project Structure

```bash
â”œâ”€â”€ airflow/                   # All DAGs and Airflow-related code
â”‚   â”œâ”€â”€ dags/
â”‚   â”‚   â””â”€â”€ my_etl_dag.py
â”‚   â”œâ”€â”€ plugins/
â”‚   â”œâ”€â”€ Dockerfile
â”‚   â””â”€â”€ requirements.txt       # Python dependencies for Airflow
â”‚
â”œâ”€â”€ config/
â”‚   â”œâ”€â”€ key.json                   # GCP service account JSON key
â”‚
â”œâ”€â”€ data/                      # Raw/local taxi trip data for batch load
â”‚   â””â”€â”€ source_data.csv
â”‚
â”œâ”€â”€ gcs/                       # GCS interaction utilities
â”‚   â”œâ”€â”€ upload_to_gcs.py
â”‚
â”œâ”€â”€ dbt/                       # dbt project for transformation
â”‚   â”œâ”€â”€ dbt_project.yml
â”‚   â”œâ”€â”€ models/
â”‚   â”‚   â””â”€â”€ transform.sql
â”‚   â””â”€â”€ profiles.yml
â”‚
â”œâ”€â”€ docker-compose.yml         # Docker setup for Airflow
â””â”€â”€ README.md
```

---

## Architecture Overview

![Pipeline Architecture](./assets/pipeline_architecture.png)

---

## How to Run

### Batch (ELT) Pipeline

1. **Start Build Docker**
   ```bash
   docker-compose build
   ```

2. **Start Airflow with Docker**
   ```bash
   docker-compose up -d
   ```

3. **Trigger DAG**
   - Open Airflow UI at `http://localhost:8080`
   - Trigger `my_etl_dag`

---

### Streaming Pipeline

1. **Create Virtual Environment & Install Dependencies**
   ```bash
   cd streaming/
   python -m venv venv
   source venv/bin/activate 
   pip install apache-beam[gcp] google-cloud-pubsub 
   ```
2. **Start ApacheBeam**
   ```bash
   python beam-pipeline-taxi.py
   ```

3. **Start Publisher**
   ```bash
   python publisher_taxi.py
   ```
---

## ğŸ› ï¸ Notes
- Ensure GCP credentials are stored securely in `config/`.
- Rename or make new folder inside `source/csv` and `source/json` based on date you run the program ex: `source/csv/2025-05-30`
- Airflow is portable via Docker but may also be installed via other methods.
- Use partitioning and incremental loading in BigQuery for optimized storage and query performance.

---
